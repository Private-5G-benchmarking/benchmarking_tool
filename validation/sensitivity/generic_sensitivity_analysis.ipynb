{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PER_PACKET_COLS = ['_time', 'packet_interarrival_time', 'packet_jitter_raw', 'packet_jitter_weighted', 'packet_owd']\n",
    "AGGREGATE_COLS  = ['_time', 'availability_2ms', 'availability_4ms', 'availability_8ms', 'availability_16ms', 'availability_32ms', 'availability_64ms', 'availability_128ms', 'packet_loss', 'throughput']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FILE_PATH = \"/home/shared/validation_backups/influx\"\n",
    "BASE_CASE_FILE_PATH = f\"{BASE_FILE_PATH}/base_case\"\n",
    "TWO_PCT_LOSS_FILE_PATH = f\"{BASE_FILE_PATH}/two_percent_pl\"\n",
    "FIFTEEN_PCT_LOSS_FILE_PATH = f\"{BASE_FILE_PATH}/fifteen_percent_pl\" \n",
    "FIVE_MS_DELAY_FILE_PATH = f\"{BASE_FILE_PATH}/five_ms_delay\"\n",
    "FIFTY_MS_DELAY_FILE_PATH = f\"{BASE_FILE_PATH}/fifty_ms_delay\"\n",
    "ONE_HUNDRED_MS_DELAY_FILE_PATH = f\"{BASE_FILE_PATH}/one_hundred_ms_delay\"\n",
    "MEDIUM_PAYLOAD_FILE_PATH = f\"{BASE_FILE_PATH}/436_byte_payload\"\n",
    "LARGE_PAYLOAD_FILE_PATH = f\"{BASE_FILE_PATH}/1432_byte_payload\"\n",
    "\n",
    "MINIMAL_BASE_FILES = [\"sensor_continuous_low_bw_long_dur_low_pl.csv\", \"cbr_low_bw_med_dur_low_pl.csv\", \"cbr_high_bw_low_dur_low_pl.csv\"]\n",
    "MINIMAL_AGGREGATE_FILES = list(map(lambda filename: f\"{filename.split('.')[0]}_aggregate.csv\", MINIMAL_BASE_FILES))\n",
    "ALL_BASE_FILES = MINIMAL_BASE_FILES + [\"sensor_continuous_low_bw_xlong_dur_low_pl.csv\", \"cbr_med_bw_med_dur_low_pl.csv\"]\n",
    "ALL_AGGREGATE_FILES = list(map(lambda filename: f\"{filename.split('.')[0]}_aggregate.csv\", ALL_BASE_FILES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamp_to_datetime(timestamp, initial_datetime):\n",
    "    \"\"\"\n",
    "    Converts a string-timestamp of ISO8601-format to Pandas datetime object,\n",
    "    normalized to 01. January 1970 \n",
    "    \"\"\"\n",
    "    delta = pd.to_datetime(timestamp, format=\"ISO8601\") - initial_datetime \n",
    "    return pd.to_datetime(delta.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_per_packet_csv(filepath):\n",
    "    \"\"\"\n",
    "    Reads a per-packet csv, selects only the relevant columns and parses the timestamp to\n",
    "    a normalized value \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath, skiprows=3)[PER_PACKET_COLS]\n",
    "    df[\"_time\"] = df[\"_time\"].apply(lambda ts: convert_timestamp_to_datetime(ts, pd.to_datetime(df[\"_time\"].iloc[0], format=\"ISO8601\")))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_aggregate_packet_csv(filepath):\n",
    "    \"\"\"\n",
    "    Reads an aggregate packet csv, selects only the relevant columns and parses the timestamp to\n",
    "    a normalized value \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath, skiprows=3)[AGGREGATE_COLS]\n",
    "    df[\"_time\"] = df[\"_time\"].apply(lambda ts: convert_timestamp_to_datetime(ts, pd.to_datetime(df[\"_time\"].iloc[0], format=\"ISO8601\")))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_column(df, col, agg_func, freq):\n",
    "    return df[[\"_time\", col]].groupby(pd.Grouper(key=\"_time\", freq=freq))\\\n",
    "                            .agg([agg_func])\\\n",
    "                            .dropna()\\\n",
    "                            .droplevel(0, axis=1)\\\n",
    "                            .rename(columns={agg_func: col})\\\n",
    "                            .reset_index()\n",
    "\n",
    "def group_and_plot_df_by_col(dfs, col, agg_func, freq):\n",
    "    dfs_aggregated = [group_by_column(df, col, agg_func, freq).rename(columns={col: f\"{col}_{index}\"}) for index, df in enumerate(dfs)]\n",
    "    new_df = dfs_aggregated[0]\n",
    "\n",
    "    for i in range(1, len(dfs_aggregated)):\n",
    "        new_df = pd.merge(new_df, dfs_aggregated[i], on=\"_time\")\n",
    "\n",
    "    return px.line(new_df, x=\"_time\", y=list(filter(lambda x: x != \"_time\", new_df.columns)), markers=True, title=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aggregate_dfs_by_col(dfs, col):\n",
    "    new_df = dfs[0][[\"_time\", col]]\n",
    "    new_df = new_df.rename(columns={col: f\"{col}_0\"})\n",
    "\n",
    "    for i in range(1, len(dfs)):\n",
    "        new_df = pd.merge(new_df, dfs[i][[\"_time\", col]], on=\"_time\").rename(columns={col: f\"{col}_{i}\"})\n",
    "\n",
    "    return px.line(new_df, x=\"_time\", y=list(filter(lambda x: x != \"_time\", new_df.columns)), markers=True, title=col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read per-packet CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_per_packet_csv(f\"{BASE_CASE_FILE_PATH}/{MINIMAL_BASE_FILES[1]}\")\n",
    "df2 = read_per_packet_csv(f\"{TWO_PCT_LOSS_FILE_PATH}/{MINIMAL_BASE_FILES[1]}\")\n",
    "df3 = read_per_packet_csv(f\"{FIFTY_MS_DELAY_FILE_PATH}/{MINIMAL_BASE_FILES[1]}\")\n",
    "# Rest of DFs comes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of per-packet KPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df, df2, df3]\n",
    "fig = group_and_plot_df_by_col(dfs, \"packet_owd\", \"mean\", \"1s\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read aggregate CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = read_aggregate_packet_csv(f\"{TWO_PCT_LOSS_FILE_PATH}/{MINIMAL_AGGREGATE_FILES[1]}\")\n",
    "df_a2 = read_aggregate_packet_csv(f\"{BASE_CASE_FILE_PATH}/{MINIMAL_AGGREGATE_FILES[1]}\")\n",
    "df_a3 = read_aggregate_packet_csv(f\"{FIFTEEN_PCT_LOSS_FILE_PATH}/{MINIMAL_AGGREGATE_FILES[1]}\")\n",
    "df_a5 = read_aggregate_packet_csv(f\"{FIFTY_MS_DELAY_FILE_PATH}/{MINIMAL_AGGREGATE_FILES[1]}\")\n",
    "df_a6 = read_aggregate_packet_csv(f\"{ONE_HUNDRED_MS_DELAY_FILE_PATH}/{MINIMAL_AGGREGATE_FILES[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of aggregate KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_a, df_a2, df_a3, df_a5, df_a6]\n",
    "fig = plot_aggregate_dfs_by_col(dfs, \"packet_loss\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
